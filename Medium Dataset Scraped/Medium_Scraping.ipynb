{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DS 203 - Scraping.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1dYOV5PeIix",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "153db608-f273-4171-e4f2-57b8a98d0d47"
      },
      "source": [
        "import numpy as np\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import random\n",
        "from textblob import TextBlob\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVAs4BHEn9SN"
      },
      "source": [
        "urls = {\n",
        "    'News':     'https://medium.com/tag/news/archive/{0}/{1:02d}/{2:02d}',\n",
        "    'Politics': 'https://medium.com/tag/politics/archive/{0}/{1:02d}/{2:02d}',\n",
        "    'Culture':  'https://medium.com/tag/culture/archive/{0}/{1:02d}/{2:02d}',\n",
        "    'Music':    'https://medium.com/tag/music/archive/{0}/{1:02d}/{2:02d}',\n",
        "    'History':  'https://medium.com/tag/history/archive/{0}/{1:02d}/{2:02d}',\n",
        "    'Journalism': 'https://medium.com/tag/journalism/archive/{0}/{1:02d}/{2:02d}',\n",
        "    'Technology': 'https://medium.com/tag/technology/archive/{0}/{1:02d}/{2:02d}',\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8AHdRtSpBhb"
      },
      "source": [
        "def is_leap(year):\n",
        "    if year % 4 != 0:\n",
        "        return False\n",
        "    elif year % 100 != 0:\n",
        "        return True\n",
        "    elif year % 400 != 0:\n",
        "        return False\n",
        "    else:\n",
        "        return True\n",
        "    \n",
        "def convert_day(day, year):\n",
        "    month_days = [31, 29 if is_leap(year) else 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
        "    m = 0\n",
        "    d = 0\n",
        "    while day > 0:\n",
        "        d = day\n",
        "        day -= month_days[m]\n",
        "        m += 1\n",
        "    return (m, d)\n",
        "\n",
        "def get_claps(claps_str):\n",
        "    if (claps_str is None) or (claps_str == '') or (claps_str.split is None):\n",
        "        return 0\n",
        "    split = claps_str.split('K')\n",
        "    claps = float(split[0])\n",
        "    claps = int(claps*1000) if len(split) == 2 else int(claps)\n",
        "    return claps\n",
        "\n",
        "def get_img(img_url, dest_folder, dest_filename):\n",
        "    ext = img_url.split('.')[-1]\n",
        "    if len(ext) > 4:\n",
        "        ext = 'jpg'\n",
        "    dest_filename = f'{dest_filename}.{ext}'\n",
        "    with open(f'{dest_folder}/{dest_filename}', 'wb') as f:\n",
        "        f.write(requests.get(img_url, allow_redirects=False).content)\n",
        "    return dest_filename\n",
        "\n",
        "def Average(lst): \n",
        "    return sum(lst) / len(lst) \n",
        "\n",
        "def return_content(url):\n",
        "  response = requests.get(url)\n",
        "  soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "  paragraphs = soup.find_all(['li', 'p', 'strong', 'em', 'h1'])\n",
        "  text = []\n",
        "  for p in paragraphs:\n",
        "        if not p.href:\n",
        "            if len(p.get_text()) > 5:\n",
        "                text.append(p.get_text())\n",
        "  \n",
        " # Number of Images\n",
        "  num_imgs = int(len(soup.find_all('img', alt=\"Image for post\")) / 3)\n",
        "\n",
        "  return ' '.join(text), num_imgs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZczkfVJ09Uj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31a03c85-271b-4d4b-b4c3-64b88aa8a629"
      },
      "source": [
        "year = 2020\n",
        "n_samples = 1\n",
        "\n",
        "# selected_days = random.sample([i for i in range(1, 367 if is_leap(year) else 366)], n_samples)\n",
        "# if year == 2020:\n",
        "#   selected_days = random.sample([i for i in range(1, 300)], n_samples)\n",
        "\n",
        "selected_days = np.arange(1, 300, 22)\n",
        "selected_days = selected_days[7:]\n",
        "print(selected_days)\n",
        "print([no % 7 for no in selected_days])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[155 177 199 221 243 265 287]\n",
            "[1, 2, 3, 4, 5, 6, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1T0xH10LG--E"
      },
      "source": [
        "def dummify(df, cate_variables):\n",
        "    '''\n",
        "    @Summary: convert the categorical variables to numeric variables by using dummies (binary).\n",
        "    Old categorical variables will be dropped.\n",
        "    @return: A copy of the old dataframe with new converted numeric variables. \n",
        "    '''\n",
        "    # make a copy before creating dummies\n",
        "    df_new = df.copy()\n",
        "    \n",
        "    # convert text-based columns to dummies (except v22)\n",
        "    for var_name in cate_variables:\n",
        "        dummies = pd.get_dummies(df[var_name], prefix=var_name)\n",
        "        \n",
        "        # Drop the current variable, concat/append the dummy dataframe to the dataframe.\n",
        "        df_new = pd.concat([df_new.drop(var_name, 1), dummies.iloc[:,1:]], axis = 1)\n",
        "    \n",
        "    return df_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSZRrWep1EKC",
        "outputId": "b01243d8-cd34-4680-8f51-ce1918347253"
      },
      "source": [
        "data = []\n",
        "article_id = 1024\n",
        "i = 0\n",
        "n = len(selected_days)\n",
        "for d in selected_days:\n",
        "    i += 1\n",
        "\n",
        "    # Date\n",
        "    month, day = convert_day(d, year)\n",
        "    date = '{0}-{1:02d}-{2:02d}'.format(year, month, day)\n",
        "    weekday = (2 + d) % 7 # 1 -> Monday\n",
        "    timedelta = 347 - d\n",
        "    print(f'{i} / {n} ; {date}')\n",
        "\n",
        "    \n",
        "    for publication, url in urls.items():\n",
        "        response = requests.get(url.format(year, month, day), allow_redirects=True)\n",
        "        if not response.url.startswith(url.format(year, month, day)):\n",
        "            continue\n",
        "        \n",
        "        # Scrape Content of the Archive Page\n",
        "        page = response.content\n",
        "        \n",
        "        # Parse the Archive Page\n",
        "        soup = BeautifulSoup(page, 'html.parser')\n",
        "        \n",
        "        # Articles in the archive\n",
        "        articles = soup.find_all(\"div\", class_=\"postArticle postArticle--short js-postArticle js-trackPostPresentation js-trackPostScrolls\")\n",
        "        for article in articles:\n",
        "            # Scrape Title\n",
        "            title = article.find(\"h3\", class_=\"graf--title\")\n",
        "            if title is None:\n",
        "                continue\n",
        "            title = title.contents[0]\n",
        "            \n",
        "            # Scrape Subtitle\n",
        "            subtitle = article.find(\"h4\", class_=\"graf--subtitle\")\n",
        "            if subtitle is None:\n",
        "                continue\n",
        "            subtitle = subtitle.contents[0] if subtitle is not None else ''\n",
        "            \n",
        "            # Scrape Title Image - Unnecessary\n",
        "            # image = article.find(\"img\", class_=\"graf-image\")\n",
        "            # image = '' if image is None else get_img(image['src'], 'images', f'{article_id}')\n",
        "            \n",
        "            # Scrape Article URL\n",
        "            article_url = article.find_all(\"a\")[3]['href'].split('?')[0]\n",
        "\n",
        "            # Get Claps\n",
        "            claps = get_claps(article.find_all(\"button\")[1].contents[0])\n",
        "            if claps == 0:\n",
        "              continue\n",
        "\n",
        "            # Scrape Content\n",
        "            text, num_imgs = return_content(article_url)\n",
        "\n",
        "            contentBlob = TextBlob(text).lower()\n",
        "                                \n",
        "            # Number of words in the content\n",
        "            n_tokens_content = len(contentBlob.words)\n",
        "            n_unique_tokens = len(list(set(contentBlob.words)))\n",
        "            \n",
        "            # article sentiment\n",
        "            content_sentiment_polarity = contentBlob.sentiment.polarity\n",
        "            content_subjectivity = contentBlob.sentiment.subjectivity \n",
        "\n",
        "            try: \n",
        "              titleBlob = TextBlob(title).lower()\n",
        "            except TypeError:\n",
        "              continue\n",
        "\n",
        "            # Language Detection\n",
        "            try:\n",
        "              lang = titleBlob.detect_language()\n",
        "            except:\n",
        "              continue\n",
        "\n",
        "            # Number of words in the title\n",
        "            n_tokens_title = len(titleBlob.words)\n",
        "\n",
        "            # title sentiment\n",
        "            title_sentiment_polarity = titleBlob.sentiment.polarity\n",
        "            title_subjectivity = titleBlob.sentiment.subjectivity\n",
        "\n",
        "            # Stop Words\n",
        "            tokens_not_sw = [word for word in contentBlob.words if word not in stopwords.words()]\n",
        "            n_non_stop_words = len(tokens_not_sw)\n",
        "            n_non_stop_unique_tokens = len(list(set(tokens_not_sw)))\n",
        "\n",
        "            # Word length\n",
        "            if len(contentBlob.words) == 0:\n",
        "              continue\n",
        "            wordlength = [len(word) for word in contentBlob.words]\n",
        "            average_token_length = Average(wordlength)\n",
        "\n",
        "            # Scrape Reading Time\n",
        "            reading_time = article.find(\"span\", class_=\"readingTime\")\n",
        "            reading_time = 0 if reading_time is None else int(reading_time['title'].split(' ')[0])\n",
        "\n",
        "            # Article ID\n",
        "            article_id += 1\n",
        "            responses = article.find_all(\"a\")\n",
        "            if len(responses) == 7:\n",
        "                responses = responses[6].contents[0].split(' ')\n",
        "                if len(responses) == 0:\n",
        "                    responses = 0\n",
        "                else:\n",
        "                    responses = responses[0]\n",
        "            else:\n",
        "                responses = 0\n",
        "\n",
        "            data.append([article_id, article_url, title, subtitle, text, num_imgs, claps, responses, reading_time, publication, date, timedelta, weekday, n_tokens_content, n_unique_tokens, content_sentiment_polarity, content_subjectivity, n_tokens_title, title_sentiment_polarity, title_subjectivity, n_non_stop_words, n_non_stop_unique_tokens, average_token_length, lang])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 / 7 ; 2020-06-03\n",
            "2 / 7 ; 2020-06-25\n",
            "3 / 7 ; 2020-07-17\n",
            "4 / 7 ; 2020-08-08\n",
            "5 / 7 ; 2020-08-30\n",
            "6 / 7 ; 2020-09-21\n",
            "7 / 7 ; 2020-10-13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjmXHxjH1HbH"
      },
      "source": [
        "medium_df = pd.DataFrame(data, columns=['ID', 'URL', 'Title', 'Subtitle', 'Content', 'Number of Images', 'Claps', 'Responses', 'Reading_time', 'Publication', 'Date', 'TimeDelta', 'Weekday', 'n_tokens_content', 'n_unique_tokens', 'content_sentiment_polarity', 'content_subjectivity', 'n_tokens_title', 'title_sentiment_polarity', 'title_subjectivity', 'n_non_stop_words', 'n_non_stop_unique_tokens', 'average_token_length', 'Language'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CO3m4Kqg7Zwb",
        "outputId": "49d5f1f9-73f2-4933-e0d6-77c3e11e54b5"
      },
      "source": [
        "medium_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>URL</th>\n",
              "      <th>Title</th>\n",
              "      <th>Subtitle</th>\n",
              "      <th>Content</th>\n",
              "      <th>Number of Images</th>\n",
              "      <th>Claps</th>\n",
              "      <th>Responses</th>\n",
              "      <th>Reading_time</th>\n",
              "      <th>Publication</th>\n",
              "      <th>Date</th>\n",
              "      <th>TimeDelta</th>\n",
              "      <th>Weekday</th>\n",
              "      <th>n_tokens_content</th>\n",
              "      <th>n_unique_tokens</th>\n",
              "      <th>content_sentiment_polarity</th>\n",
              "      <th>content_subjectivity</th>\n",
              "      <th>n_tokens_title</th>\n",
              "      <th>title_sentiment_polarity</th>\n",
              "      <th>title_subjectivity</th>\n",
              "      <th>n_non_stop_words</th>\n",
              "      <th>n_non_stop_unique_tokens</th>\n",
              "      <th>average_token_length</th>\n",
              "      <th>Language</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1025</td>\n",
              "      <td>https://blog.gojekengineering.com/global-techn...</td>\n",
              "      <td>Global technology and payments companies inves...</td>\n",
              "      <td>The investment will boost Southeast Asia’s…</td>\n",
              "      <td>Culture Design Stories We're Hiring!  Global t...</td>\n",
              "      <td>2</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>News</td>\n",
              "      <td>2020-06-03</td>\n",
              "      <td>192</td>\n",
              "      <td>3</td>\n",
              "      <td>1130</td>\n",
              "      <td>420</td>\n",
              "      <td>0.184232</td>\n",
              "      <td>0.386133</td>\n",
              "      <td>8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>685</td>\n",
              "      <td>359</td>\n",
              "      <td>5.120354</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1026</td>\n",
              "      <td>https://medium.com/discourse/the-libertarian-t...</td>\n",
              "      <td>The Libertarian to Fascist Pipeline is Shorter...</td>\n",
              "      <td>Though opposing ideologies, Libertarians…</td>\n",
              "      <td>Politics Economy Energy &amp; Climate Technology S...</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>News</td>\n",
              "      <td>2020-06-03</td>\n",
              "      <td>192</td>\n",
              "      <td>3</td>\n",
              "      <td>2055</td>\n",
              "      <td>738</td>\n",
              "      <td>0.139039</td>\n",
              "      <td>0.458405</td>\n",
              "      <td>10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1129</td>\n",
              "      <td>623</td>\n",
              "      <td>5.016058</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1027</td>\n",
              "      <td>https://medium.com/@hcsiratt/a-plea-for-unity-...</td>\n",
              "      <td>A Plea for Unity and Peace</td>\n",
              "      <td>In times of crisis, we must unite as a community</td>\n",
              "      <td>A Plea for Unity and Peace The recent death of...</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>News</td>\n",
              "      <td>2020-06-03</td>\n",
              "      <td>192</td>\n",
              "      <td>3</td>\n",
              "      <td>1782</td>\n",
              "      <td>640</td>\n",
              "      <td>-0.000158</td>\n",
              "      <td>0.512571</td>\n",
              "      <td>6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>865</td>\n",
              "      <td>537</td>\n",
              "      <td>4.602694</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1028</td>\n",
              "      <td>https://medium.com/quatria/is-this-cave-painti...</td>\n",
              "      <td>Is This Cave Painting A Prehistoric Map To Ant...</td>\n",
              "      <td>Scientists say this old drawing contains an…</td>\n",
              "      <td>Is This Cave Painting A Prehistoric Map To Ant...</td>\n",
              "      <td>1</td>\n",
              "      <td>53</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>News</td>\n",
              "      <td>2020-06-03</td>\n",
              "      <td>192</td>\n",
              "      <td>3</td>\n",
              "      <td>472</td>\n",
              "      <td>264</td>\n",
              "      <td>0.111264</td>\n",
              "      <td>0.356929</td>\n",
              "      <td>9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>269</td>\n",
              "      <td>211</td>\n",
              "      <td>5.076271</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1029</td>\n",
              "      <td>https://medium.com/@ypi_78836/read-news-read-c...</td>\n",
              "      <td>Read News, Read China, an NLP approach</td>\n",
              "      <td>Tracking the polices change from the enormous ...</td>\n",
              "      <td>Read News, Read China, an NLP approach Thanks ...</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>News</td>\n",
              "      <td>2020-06-03</td>\n",
              "      <td>192</td>\n",
              "      <td>3</td>\n",
              "      <td>1098</td>\n",
              "      <td>494</td>\n",
              "      <td>0.076150</td>\n",
              "      <td>0.350728</td>\n",
              "      <td>7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>646</td>\n",
              "      <td>422</td>\n",
              "      <td>5.030055</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1550</th>\n",
              "      <td>2575</td>\n",
              "      <td>https://medium.com/an-idea/the-e-reader-is-a-p...</td>\n",
              "      <td>The E-reader Is a Powerful Tool for Writers</td>\n",
              "      <td>An E-Reader Is a Writer’s Card File</td>\n",
              "      <td>Life, Health &amp; Nutrition Business &amp; Travel Sci...</td>\n",
              "      <td>1</td>\n",
              "      <td>53</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>Technology</td>\n",
              "      <td>2020-10-13</td>\n",
              "      <td>60</td>\n",
              "      <td>2</td>\n",
              "      <td>981</td>\n",
              "      <td>348</td>\n",
              "      <td>0.198784</td>\n",
              "      <td>0.544020</td>\n",
              "      <td>8</td>\n",
              "      <td>0.3</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>445</td>\n",
              "      <td>269</td>\n",
              "      <td>4.126402</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1551</th>\n",
              "      <td>2576</td>\n",
              "      <td>https://medium.com/callforcode/a-global-crisis...</td>\n",
              "      <td>A global crisis needs a global response</td>\n",
              "      <td>Mami Mizutori, UNDRR talks about what it will ...</td>\n",
              "      <td>Regional Wrap-Up Series A global crisis needs ...</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Technology</td>\n",
              "      <td>2020-10-13</td>\n",
              "      <td>60</td>\n",
              "      <td>2</td>\n",
              "      <td>429</td>\n",
              "      <td>230</td>\n",
              "      <td>0.228416</td>\n",
              "      <td>0.403002</td>\n",
              "      <td>7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>249</td>\n",
              "      <td>183</td>\n",
              "      <td>4.780886</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1552</th>\n",
              "      <td>2577</td>\n",
              "      <td>https://medium.com/illumination/backlinks-impo...</td>\n",
              "      <td>Backlinks Important For SEO But You Don’t Need...</td>\n",
              "      <td>I can’t promise your blog the first position…</td>\n",
              "      <td>Business Poetry Fiction Philosophy Science Tec...</td>\n",
              "      <td>1</td>\n",
              "      <td>56</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>Technology</td>\n",
              "      <td>2020-10-13</td>\n",
              "      <td>60</td>\n",
              "      <td>2</td>\n",
              "      <td>916</td>\n",
              "      <td>322</td>\n",
              "      <td>0.135354</td>\n",
              "      <td>0.495042</td>\n",
              "      <td>12</td>\n",
              "      <td>0.4</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>466</td>\n",
              "      <td>244</td>\n",
              "      <td>4.529476</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1553</th>\n",
              "      <td>2578</td>\n",
              "      <td>https://medium.com/genius-in-a-bottle/i-found-...</td>\n",
              "      <td>I Found My Lost Attention</td>\n",
              "      <td>A poem</td>\n",
              "      <td>Submission Guidelines Prompt Guidelines Archiv...</td>\n",
              "      <td>1</td>\n",
              "      <td>510</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Technology</td>\n",
              "      <td>2020-10-13</td>\n",
              "      <td>60</td>\n",
              "      <td>2</td>\n",
              "      <td>138</td>\n",
              "      <td>89</td>\n",
              "      <td>0.425000</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>85</td>\n",
              "      <td>65</td>\n",
              "      <td>4.934783</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1554</th>\n",
              "      <td>2579</td>\n",
              "      <td>https://medium.com/datadriveninvestor/artifici...</td>\n",
              "      <td>Artificial intelligence brings us back to the ...</td>\n",
              "      <td>Putting sanity back into a crazy world</td>\n",
              "      <td>Blockchain Finance Economics Startup Artificia...</td>\n",
              "      <td>1</td>\n",
              "      <td>56</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>Technology</td>\n",
              "      <td>2020-10-13</td>\n",
              "      <td>60</td>\n",
              "      <td>2</td>\n",
              "      <td>1181</td>\n",
              "      <td>480</td>\n",
              "      <td>0.167150</td>\n",
              "      <td>0.490552</td>\n",
              "      <td>9</td>\n",
              "      <td>-0.2</td>\n",
              "      <td>0.366667</td>\n",
              "      <td>706</td>\n",
              "      <td>392</td>\n",
              "      <td>4.813717</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1555 rows × 24 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        ID  ... Language\n",
              "0     1025  ...       en\n",
              "1     1026  ...       en\n",
              "2     1027  ...       en\n",
              "3     1028  ...       en\n",
              "4     1029  ...       en\n",
              "...    ...  ...      ...\n",
              "1550  2575  ...       en\n",
              "1551  2576  ...       en\n",
              "1552  2577  ...       en\n",
              "1553  2578  ...       en\n",
              "1554  2579  ...       en\n",
              "\n",
              "[1555 rows x 24 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlHa2DFV6s86"
      },
      "source": [
        "medium_df.to_csv('medium_data.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiT7m8TzIIad"
      },
      "source": [
        "# url = 'https://medium.com/unifiprotocol/unifi-protocol-is-launching-on-ethereum-5501f19c4b6'\n",
        "# response = requests.get(url)\n",
        "# soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# paragraphs = soup.find_all(['li', 'p', 'strong', 'em', 'h1'])\n",
        "# text = []\n",
        "# for p in paragraphs:\n",
        "#       if not p.href:\n",
        "#           if len(p.get_text()) > 5:\n",
        "#               text.append(p.get_text())\n",
        "\n",
        "# num_imgs = int(len(soup.findAll('img', alt=\"Image for post\")) / 3)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}