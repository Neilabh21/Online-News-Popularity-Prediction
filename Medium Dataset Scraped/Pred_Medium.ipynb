{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pred_Medium.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "113X0GRdGxuq",
        "dWzo5BSnJE2I",
        "LlDPCh4WexmV",
        "8mzmwnN5f7h1"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmEbXWeIoJ6f"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "113X0GRdGxuq"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDUCHIzaKQai",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "ca3d8c19-716e-4345-de60-b9de2347d023"
      },
      "source": [
        "df1 = pd.read_csv('Medium - Final.csv')\r\n",
        "df2 = df1\r\n",
        "df2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-cbe5a7b2e2ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Medium - Final.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Medium - Final.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7f7OLdpeUE7a"
      },
      "source": [
        "# Finding median of the 'Claps' column\r\n",
        "# Here Claps is a metric of popularity\r\n",
        "df2['Claps'].median()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aCpS5f5JE8G"
      },
      "source": [
        "for row in range(df1.shape[0]):\r\n",
        "  if df1['Claps'][row] < 99:\r\n",
        "    df2['Claps'][row] = 0\r\n",
        "  else :\r\n",
        "    df2['Claps'][row] = 1\r\n",
        "# The article with claps more than 99 are considered popular, else they are labelled unpopular\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DSZ76F-0YWF"
      },
      "source": [
        "for column in df2:\r\n",
        "  print(\"The column '\"+str(column)+\"' is of the type '\"+str(df2[column].dtypes)+\"', has '\"+str(len(df2[column].unique()))+\"' unique values, and has '\"+str(np.sum(pd.isnull(df2[column])))+\"' null values\")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7s5NYy37K-i"
      },
      "source": [
        "# converting Responses column(string) to integer\r\n",
        "# errors = 'coerce' sets the invalid parsing as NaN\r\n",
        "df2['Responses'] = pd.to_numeric(df2['Responses'],errors='coerce')\r\n",
        "\r\n",
        "# Changing structure of the date variable\r\n",
        "for row in range(df2.shape[0]):\r\n",
        "  date = df2['Date'][row]\r\n",
        "  try:\r\n",
        "    date = date.split('-')\r\n",
        "    df2.replace({'Date' : {df2['Date'][row]:int(date[0])*10000 + int(date[1])*100 + int(date[2])}},inplace=True)\r\n",
        "\r\n",
        "  except:\r\n",
        "    continue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zV_ry_pr75-Y"
      },
      "source": [
        "for column in df2:\r\n",
        "  print(\"The column '\"+str(column)+\"' is of the type '\"+str(df2[column].dtypes)+\"', has '\"+str(len(df2[column].unique()))+\"' unique values, and has '\"+str(np.sum(pd.isnull(df2[column])))+\"' null values\")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyaBZb_qfwb4"
      },
      "source": [
        "# Input variables\r\n",
        "# Here the url is ignored in the first column\r\n",
        "X = np.column_stack((df2.iloc[:,5].values,df2.iloc[:,7:].values))\r\n",
        "# Target variable\r\n",
        "y = df2.iloc[:,6].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3rFGLsQ0XXN"
      },
      "source": [
        "# One - hot encoding\r\n",
        "from sklearn.compose import ColumnTransformer\r\n",
        "from sklearn.preprocessing import OneHotEncoder\r\n",
        "\r\n",
        "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [3,-1])], remainder='passthrough')\r\n",
        "\r\n",
        "X = np.array(ct.fit_transform(X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsvPZ2t9AuxK"
      },
      "source": [
        "y[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VoEtJPpha6T"
      },
      "source": [
        "# Splitting data (train:validation:text :: 70:15:15)\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X,y, test_size = 0.15, random_state = 0)\r\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_temp,y_temp, test_size = 0.1764, random_state = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Clkco6vdhaz9"
      },
      "source": [
        "# Normalisation\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "sc = StandardScaler()\r\n",
        "\r\n",
        "X_train = sc.fit_transform(X_train)\r\n",
        "# Normalising X_valid and Normalising X_test with the same mean and variance used to normalising X_train\r\n",
        "X_test = sc.transform(X_test)\r\n",
        "X_valid = sc.transform(X_valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWzo5BSnJE2I"
      },
      "source": [
        "## Logistic Rgression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04WOvPLUJEuA"
      },
      "source": [
        "# Training\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "classifier = LogisticRegression()\r\n",
        "\r\n",
        "classifier.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qb8mRiL8exgQ"
      },
      "source": [
        "# Accuracy and the confusion matrix\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "y_valid_pred = classifier.predict(X_valid)\r\n",
        "cm = confusion_matrix(y_valid, y_valid_pred)\r\n",
        "\r\n",
        "print('Accuracy:',classifier.score(X_valid,y_valid))\r\n",
        "print('Confusion Matrix', cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bv0YI35nM91M"
      },
      "source": [
        "# Using different regularisation strengths\r\n",
        "\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from google.colab import widgets\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "\r\n",
        "\r\n",
        "accu1 = []\r\n",
        "tb2 = widgets.TabBar([str(i) for i in ['C = 0.1','C = 0.5','C = 1','C = 1.5']])\r\n",
        "C = [0.1,0.5,1,1.5]\r\n",
        "for i in range(4):\r\n",
        "  with tb2.output_to(i):\r\n",
        "    # Splitting\r\n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(X,y, test_size = 0.15, random_state = 0)\r\n",
        "    X_train, X_valid, y_train, y_valid = train_test_split(X_temp,y_temp, test_size = 0.1764, random_state = 0)\r\n",
        "\r\n",
        "    # Normalisation\r\n",
        "    sc = StandardScaler()\r\n",
        "    X_train = sc.fit_transform(X_train)\r\n",
        "    X_valid = sc.transform(X_valid)\r\n",
        "\r\n",
        "    # Training\r\n",
        "    classifier = LogisticRegression(C = C[i],tol = 1e-3)\r\n",
        "    classifier.fit(X_train,y_train)\r\n",
        "\r\n",
        "    # Predicting\r\n",
        "    y_valid_pred = classifier.predict(X_valid)\r\n",
        "\r\n",
        "    # Error Measurement\r\n",
        "    cm = confusion_matrix(y_valid, y_valid_pred)\r\n",
        "    accu1.append(classifier.score(X_valid,y_valid))\r\n",
        "    print('Accuracy:',classifier.score(X_valid,y_valid))\r\n",
        "    print('Confusion Matrix', cm)\r\n",
        "\r\n",
        "\r\n",
        "# Here different tabs corresponds to different hyperparameters\r\n",
        "# In this case it is the Regularisation parameter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qzuCVQKZQNy"
      },
      "source": [
        "sb.lineplot(x = [0.1,0.5,1,1.5], y = accu1)\r\n",
        "plt.xlabel('Regularisation Parameter',fontsize=16)\r\n",
        "plt.ylabel('Accuracy',fontsize=16)\r\n",
        "plt.title('Accuracy vs regularisation parameter(Logistic Reg.)',fontsize = 16);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlDPCh4WexmV"
      },
      "source": [
        "## SVC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zbq66ue8f7OS"
      },
      "source": [
        "# Training\r\n",
        "from sklearn.svm import SVC\r\n",
        "\r\n",
        "classifier = SVC(kernel='rbf')\r\n",
        "classifier.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85NWnDymf7bF"
      },
      "source": [
        "# Accuracy and the confusion matrix\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "y_valid_pred = classifier.predict(X_valid)\r\n",
        "cm = confusion_matrix(y_valid, y_valid_pred)\r\n",
        "\r\n",
        "print('Accuracy:',classifier.score(X_valid,y_valid))\r\n",
        "print('Confusion Matrix', cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLc0B82zTn1L"
      },
      "source": [
        "# Using different regularisation strengths\r\n",
        "\r\n",
        "from sklearn.svm import SVC\r\n",
        "from google.colab import widgets\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "\r\n",
        "accu2 = []\r\n",
        "tb2 = widgets.TabBar([str(i) for i in ['C = 1','C = 5','C = 10','C = 20','C = 30']])\r\n",
        "C = [1,5,10,20,30]\r\n",
        "for i in range(5):\r\n",
        "  with tb2.output_to(i):\r\n",
        "    # Splitting\r\n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(X,y, test_size = 0.15, random_state = 0)\r\n",
        "    X_train, X_valid, y_train, y_valid = train_test_split(X_temp,y_temp, test_size = 0.1764, random_state = 0)\r\n",
        "\r\n",
        "    # Normalisation\r\n",
        "    sc = StandardScaler()\r\n",
        "    X_train = sc.fit_transform(X_train)\r\n",
        "    X_valid = sc.transform(X_valid)\r\n",
        "\r\n",
        "    # Training\r\n",
        "    classifier = SVC(C = C[i])\r\n",
        "    classifier.fit(X_train,y_train)\r\n",
        "\r\n",
        "    # Predicting\r\n",
        "    y_valid_pred = classifier.predict(X_valid)\r\n",
        "\r\n",
        "    # Error Measurement\r\n",
        "    cm = confusion_matrix(y_valid, y_valid_pred)\r\n",
        "    accu2.append(classifier.score(X_valid,y_valid))\r\n",
        "    print('Accuracy:',classifier.score(X_valid,y_valid))\r\n",
        "    print('Confusion Matrix', cm)\r\n",
        "\r\n",
        "\r\n",
        "# Here different tabs corresponds to different hyperparameters\r\n",
        "# In this case it is the Regularisation parameter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1sbh-nAa35R"
      },
      "source": [
        "sb.lineplot(x = [1,5,10,20,30], y = accu2)\r\n",
        "plt.xlabel('Regularisation Parameter',fontsize=16)\r\n",
        "plt.ylabel('Accuracy',fontsize=16)\r\n",
        "plt.title('Accuracy vs regularisation parameter(SVC)',fontsize = 16);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mzmwnN5f7h1"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71g1THrWgLgD"
      },
      "source": [
        "# Training\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "\r\n",
        "classifier = RandomForestClassifier(n_estimators = 100, random_state = 0)\r\n",
        "classifier.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z45mgfplgLqv"
      },
      "source": [
        "# Accuracy and the confusion matrix\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "y_valid_pred = classifier.predict(X_valid)\r\n",
        "cm = confusion_matrix(y_valid, y_valid_pred)\r\n",
        "\r\n",
        "print('Accuracy:',classifier.score(X_valid,y_valid))\r\n",
        "print('Confusion Matrix', cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68lZi2L8T19A"
      },
      "source": [
        "# Using different number of trees\r\n",
        "\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from google.colab import widgets\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "\r\n",
        "accu3 = []\r\n",
        "tb2 = widgets.TabBar([str(i) for i in ['n_estimators = 10','n_estimators = 50','n_estimators = 100','n_estimators = 200','n_estimators = 300']])\r\n",
        "n_estimators = [10,50,100,200,300]\r\n",
        "for i in range(5):\r\n",
        "  with tb2.output_to(i):\r\n",
        "    # Splitting\r\n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(X,y, test_size = 0.15, random_state = 0)\r\n",
        "    X_train, X_valid, y_train, y_valid = train_test_split(X_temp,y_temp, test_size = 0.1764, random_state = 0)\r\n",
        "\r\n",
        "\r\n",
        "    # Training\r\n",
        "    classifier = RandomForestClassifier(n_estimators = n_estimators[i])\r\n",
        "    classifier.fit(X_train,y_train)\r\n",
        "    \r\n",
        "\r\n",
        "    # Predicting\r\n",
        "    y_valid_pred = classifier.predict(X_valid)\r\n",
        "\r\n",
        "    # Error Measurement\r\n",
        "    cm = confusion_matrix(y_valid, y_valid_pred)\r\n",
        "    accu3.append(classifier.score(X_valid,y_valid))\r\n",
        "    print('Accuracy:',classifier.score(X_valid,y_valid))\r\n",
        "    print('Confusion Matrix', cm)\r\n",
        "\r\n",
        "\r\n",
        "# Here different tabs corresponds to different hyperparameters\r\n",
        "# In this case, it is the number of trees in the forest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5fhAboLa4Kq"
      },
      "source": [
        "sb.lineplot(x = [10,50,100,200,300], y = accu3)\r\n",
        "plt.xlabel('Number of Trees',fontsize=16)\r\n",
        "plt.ylabel('Accuracy',fontsize=16)\r\n",
        "plt.title('Accuracy vs regularisation parameter(R.F.)',fontsize = 16);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIaaxQl633BW"
      },
      "source": [
        "## NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98kPaMZpuuS9"
      },
      "source": [
        "import seaborn as sb\r\n",
        "import torch\r\n",
        "import torch.nn as nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtSqZQ93uuYG"
      },
      "source": [
        "# Setting up the data types and device to be used\r\n",
        "dtype = torch.float\r\n",
        "device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BshNwg_uudC"
      },
      "source": [
        "# Dataset module for the dataset\r\n",
        "class mydataset(torch.utils.data.Dataset):\r\n",
        "  def __init__(self,input,labels):\r\n",
        "    super(mydataset,self).__init__()\r\n",
        "    self.x = torch.from_numpy(input)\r\n",
        "    self.y = torch.from_numpy(labels)\r\n",
        "    self.n_samples = self.x.shape[0]\r\n",
        "\r\n",
        "  def __getitem__(self,index):\r\n",
        "       return self.x[index], self.y[index]\r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "    return self.n_samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAeHv0s1Fu6p"
      },
      "source": [
        "X_train = X_train.astype(None)\r\n",
        "X_valid = X_valid.astype(None)\r\n",
        "X_test = X_test.astype(None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flMvG6eluuh2"
      },
      "source": [
        "train_data = mydataset(X_train,y_train)\r\n",
        "valid_data = mydataset(X_valid,y_valid)\r\n",
        "test_data = mydataset(X_test,y_test)\r\n",
        "\r\n",
        "# We want to include the last batch as well so we will set drop_last as False\r\n",
        "# We only need shuffle in the train data set as it help with the training\r\n",
        "train_loader = torch.utils.data.DataLoader(dataset= train_data, batch_size= 50)\r\n",
        "valid_loader = torch.utils.data.DataLoader(dataset= valid_data, batch_size= 50,\r\n",
        "                                           shuffle = True, drop_last = False)\r\n",
        "test_loader = torch.utils.data.DataLoader(dataset= test_data, batch_size= 50,\r\n",
        "                                          shuffle = True, drop_last = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeQ6lsMsxuBG"
      },
      "source": [
        "input_size = 46\r\n",
        "hidden_size = 4\r\n",
        "num_classes = 2\r\n",
        "\r\n",
        "\r\n",
        "class mymodel(nn.Module):\r\n",
        "  def __init__(self,input_size, num_classes,hidden_size=4):\r\n",
        "    super(mymodel, self).__init__()\r\n",
        "    self.l1 = nn.Linear(input_size,hidden_size)\r\n",
        "    self.relu = nn.ReLU()\r\n",
        "    self.l2 = nn.Linear(hidden_size,num_classes)\r\n",
        "  \r\n",
        "  def forward(self,x):\r\n",
        "    out = self.l1(x)\r\n",
        "    out = self.relu(out)\r\n",
        "    out = self.l2(out)\r\n",
        "    return out\r\n",
        "  \r\n",
        "  \r\n",
        "model = mymodel(input_size,num_classes,hidden_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vf01T1qbxuIq"
      },
      "source": [
        "# loss and optimizer\r\n",
        "learning_rate = 1e-3\r\n",
        "\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "criterion2 = nn.MSELoss()\r\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYoZYtz3xuPI"
      },
      "source": [
        "# learning loop\r\n",
        "n_total_steps = len(train_loader)\r\n",
        "num_epochs = 200\r\n",
        "\r\n",
        "train_loss = []\r\n",
        "train_accuracy = []\r\n",
        "valid_loss = []\r\n",
        "valid_accuracy = []\r\n",
        "\r\n",
        "for epoch in range(num_epochs):\r\n",
        "  for input, labels in train_loader:\r\n",
        "\r\n",
        "    input = input.to(device) # to push the input to device\r\n",
        "    labels = labels.to(device) # to push the labels to device\r\n",
        "\r\n",
        "    # forward pass\r\n",
        "    outputs = model.forward(input.float())\r\n",
        "    loss = criterion(outputs, labels)\r\n",
        "    \r\n",
        "    # backward pass\r\n",
        "    optimizer.zero_grad() # to empty the value in the gradient attribute\r\n",
        "    loss.backward() # for backpropogation\r\n",
        "    optimizer.step() # to update the parameters\r\n",
        "\r\n",
        "  # Loss for training set\r\n",
        "  with torch.no_grad():\r\n",
        "    output_train = model(torch.from_numpy(X_train).float())\r\n",
        "    loss_train = criterion(output_train,torch.from_numpy(y_train))\r\n",
        "\r\n",
        "  # Accuracy for training set\r\n",
        "  _, predict_train = torch.max(output_train,1)\r\n",
        "  n_samples = torch.from_numpy(y_train).shape[0]\r\n",
        "  n_correct = (predict_train == torch.from_numpy(y_train)).sum().item()\r\n",
        "\r\n",
        "  train_loss.append(loss_train.item())\r\n",
        "  train_accuracy.append(100 * n_correct / n_samples)\r\n",
        "\r\n",
        "  # Loss for validation set\r\n",
        "  with torch.no_grad():\r\n",
        "    output_valid = model(torch.from_numpy(X_valid).float())\r\n",
        "    loss_valid = criterion(output_valid,torch.from_numpy(y_valid))\r\n",
        "\r\n",
        "  # Accuracy for validation set\r\n",
        "  _, predict_valid = torch.max(output_valid,1) # not interested in the first value\r\n",
        "  n_samples = torch.from_numpy(y_valid).shape[0]\r\n",
        "  n_correct = (predict_valid == torch.from_numpy(y_valid)).sum().item()\r\n",
        "\r\n",
        "  \r\n",
        "  valid_loss.append(loss_valid.item())\r\n",
        "  valid_accuracy.append(100 * n_correct / n_samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHPekpb41QBK"
      },
      "source": [
        "### 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4msNJQRt1Tmn"
      },
      "source": [
        "#### (a)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhAgzHj-1JcB"
      },
      "source": [
        "sb.lineplot(x = range(1,201), y = train_loss)\n",
        "sb.lineplot(x = range(1,201), y = valid_loss)\n",
        "plt.legend(['Training Loss','Validation Loss'])\n",
        "plt.xlabel('Number of epochs',fontsize=16)\n",
        "plt.ylabel('Loss',fontsize=16)\n",
        "plt.title('Training and Validation loss vs epoch',fontsize = 20);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r89DLZMx1Xnh"
      },
      "source": [
        "#### (b)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0rvsmZM1jlO"
      },
      "source": [
        "sb.lineplot(x = range(1,201), y = train_accuracy)\n",
        "sb.lineplot(x = range(1,201), y = valid_accuracy)\n",
        "plt.legend(['Training Accuracy','Validation Accuracy'])\n",
        "plt.xlabel('Number of epochs',fontsize=16)\n",
        "plt.ylabel('Accuracy',fontsize=16)\n",
        "plt.title('Training and Validation Accuracy vs epoch',fontsize = 20);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6m5Hf-51aYP"
      },
      "source": [
        "#### (c)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XCaUSHR1mkZ"
      },
      "source": [
        "# Validation Loss vs Hidden layer size\n",
        "\n",
        "#for hidden_size in [20,30,40,50,60]:\n",
        "for hidden_size in [20,30,40,50,60]:\n",
        "  new_model = mymodel(input_size,num_classes,hidden_size)\n",
        "\n",
        "  # loss and optimizer\n",
        "  learning_rate = 1e-3\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.SGD(new_model.parameters(), lr = learning_rate)\n",
        "\n",
        "  # learning loop\n",
        "  n_total_steps = len(train_loader)\n",
        "  num_epochs = 100\n",
        "  valid_loss = []\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    for input, labels in train_loader:\n",
        "      input = input.to(device) # to push the input to device\n",
        "      labels = labels.to(device) # to push the labels to device\n",
        "\n",
        "      # forward pass\n",
        "      outputs = new_model.forward(input.float())\n",
        "      loss = criterion(outputs, labels)\n",
        "    \n",
        "    \n",
        "      # backward pass\n",
        "      optimizer.zero_grad() # to empty the value in the gradient attribute\n",
        "      loss.backward() # for backpropogation\n",
        "      optimizer.step() # to update the parameters\n",
        "\n",
        "\n",
        "    # Loss for validation set\n",
        "    with torch.no_grad():\n",
        "      output_valid = new_model(torch.from_numpy(X_train).float())\n",
        "      loss_valid = criterion(output_valid,torch.from_numpy(y_train))\n",
        "\n",
        "  \n",
        "    valid_loss.append(loss_valid.item())\n",
        "    \n",
        "  sb.lineplot(x = range(1,101), y = valid_loss)\n",
        "\n",
        "plt.legend(['Layer size = 20','Layer size = 30','Layer size = 40','Layer size = 50',\n",
        "            'Layer size = 60'])\n",
        "plt.xlabel('Number of epochs',fontsize=16)\n",
        "plt.ylabel('Validation Loss',fontsize=16)\n",
        "plt.title('Validation loss vs Hidden Layer size',fontsize = 20);\n",
        "# Graph shows that the neural network with 50 hidden layers has highest decline \n",
        "# in the validation loss (for a common number of epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owp8HuTW1d2l"
      },
      "source": [
        "#### (d)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf_ax0nCUS2h"
      },
      "source": [
        "# Validation Loss vs Learning rate\n",
        "\n",
        "\n",
        "for learning_rate in [1e-5,1e-4,1e-3,1e-2,1e-1]:\n",
        "  hidden_size = 50 # best layer size\n",
        "  new_model = mymodel(input_size,num_classes,hidden_size)\n",
        "\n",
        "  # loss and optimizer\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.SGD(new_model.parameters(), lr = learning_rate)\n",
        "\n",
        "  # learning loop\n",
        "  n_total_steps = len(train_loader)\n",
        "  num_epochs = 100\n",
        "  valid_loss = []\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    for input, labels in train_loader:\n",
        "      input = input.to(device) # to push the input to device\n",
        "      labels = labels.to(device) # to push the labels to device\n",
        "\n",
        "      # forward pass\n",
        "      outputs = new_model.forward(input.float())\n",
        "      loss = criterion(outputs, labels)\n",
        "    \n",
        "    \n",
        "      # backward pass\n",
        "      optimizer.zero_grad() # to empty the value in the gradient attribute\n",
        "      loss.backward() # for backpropogation\n",
        "      optimizer.step() # to update the parameters\n",
        "\n",
        "\n",
        "    # Loss for validation set\n",
        "    with torch.no_grad():\n",
        "      output_valid = new_model(torch.from_numpy(X_train).float())\n",
        "      loss_valid = criterion(output_valid,torch.from_numpy(y_train))\n",
        "\n",
        "  \n",
        "    valid_loss.append(loss_valid.item())\n",
        "    #print('epoch: ',epoch+1,'/',num_epochs,'|| Valid loss: ',\n",
        "    #      np.round(loss_valid.item(),4))\n",
        "  \n",
        "  sb.lineplot(x = range(1,101), y = valid_loss)\n",
        "\n",
        "plt.legend(['Learning Rate = 1e-5','Learning Rate = 1e-4','Learning Rate = 1e-3',\n",
        "            'Learning Rate = 1e-2','Learning Rate = 1e-1'])\n",
        "plt.xlabel('Number of epochs',fontsize=16)\n",
        "plt.ylabel('Validation Loss',fontsize=16)\n",
        "plt.title('Validation loss vs Learning Rate',fontsize = 20);\n",
        "# Graph shows that the neural network with learning rate as 1e-1 has highest \n",
        "# decline in the validation loss (for a common number of epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-XMQ8ZRxuVb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}